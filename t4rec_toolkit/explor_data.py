# === PHASE 1 : EXPLORATION ET VALIDATION - DATAIKU PARTITIONN√â ===
# Analyse de BASE_SCORE_COMPLETE_prepared pour d√©ploiement T4Rec sur 2024

import dataiku
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import warnings

warnings.filterwarnings("ignore")

print("üîç PHASE 1 : EXPLORATION BASE_SCORE_COMPLETE_prepared")
print("=" * 70)

# === CONNEXION DATASETS DATAIKU ===
print("\nüìä 1. CONNEXION AUX DATASETS...")

# Input : Table partitionn√©e source
input_dataset = dataiku.Dataset("BASE_SCORE_COMPLETE_prepared")

# Output : R√©sultats d'analyse (√† cr√©er dans Dataiku)
output_dataset = dataiku.Dataset("T4REC_ANALYSIS_PHASE1")

# === ANALYSE PARTITIONS DISPONIBLES ===
print("\nüìÖ 2. ANALYSE DES PARTITIONS...")

try:
    # R√©cup√©rer les partitions disponibles
    partitions = input_dataset.list_partitions()
    print(f"‚úÖ Partitions disponibles: {len(partitions)}")

    # Filtrer les partitions 2024
    partitions_2024 = [p for p in partitions if "2024" in p]
    print(f"‚úÖ Partitions 2024: {len(partitions_2024)}")
    print(f"üìã D√©tail 2024: {partitions_2024}")

except Exception as e:
    print(f"‚ö†Ô∏è Erreur partitions: {e}")
    # Fallback : prendre un √©chantillon
    partitions_2024 = ["2024-01", "2024-02", "2024-03"]  # √Ä adapter selon votre naming

# === √âCHANTILLONNAGE INTELLIGENT ===
print("\nüî¨ 3. √âCHANTILLONNAGE POUR ANALYSE...")


def sample_partition_data(partition_id, sample_size=10000):
    """√âchantillonne une partition de mani√®re optimis√©e"""
    try:
        # Lire avec limite pour √©viter l'overflow m√©moire
        df_sample = input_dataset.get_dataframe(
            partition=partition_id, limit=sample_size
        )
        return df_sample, len(df_sample)
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur partition {partition_id}: {e}")
        return None, 0


# Analyser 3 partitions repr√©sentatives
sample_partitions = (
    partitions_2024[:3] if len(partitions_2024) >= 3 else partitions_2024
)
analysis_results = {}

total_estimated_size = 0
df_structure_sample = None

for partition in sample_partitions:
    print(f"   üìä Analyse partition: {partition}")
    df_sample, size = sample_partition_data(partition, sample_size=5000)

    if df_sample is not None:
        analysis_results[partition] = {
            "rows_sampled": size,
            "columns_count": len(df_sample.columns),
            "memory_mb": df_sample.memory_usage(deep=True).sum() / 1024 / 1024,
        }

        # Garder un √©chantillon pour analyse structure
        if df_structure_sample is None:
            df_structure_sample = df_sample.copy()

        # Estimation taille totale (approximation)
        if size > 0:
            # Estimer le ratio √©chantillon/r√©el bas√© sur la taille
            estimated_full_size = size * 20  # Hypoth√®se : √©chantillon = 5% du total
            total_estimated_size += estimated_full_size
            analysis_results[partition]["estimated_total_rows"] = estimated_full_size

print(f"‚úÖ √âchantillonnage termin√© sur {len(analysis_results)} partitions")

# === ANALYSE STRUCTURE DES DONN√âES ===
print("\nüèóÔ∏è 4. ANALYSE STRUCTURE DES DONN√âES...")

if df_structure_sample is not None:
    print(f"üìä Dataset √©chantillon: {df_structure_sample.shape}")

    # === COLONNES DISPONIBLES ===
    all_columns = list(df_structure_sample.columns)
    print(f"üìã Total colonnes: {len(all_columns)}")

    # === COMPARAISON AVEC LE TEST PR√âC√âDENT ===
    # Colonnes utilis√©es dans votre test pipeline_t4rec_training.py
    expected_sequence_cols = [
        "nbchqemigliss_m12",
        "nb_automobile_12dm",
        "mntecscrdimm",
        "mnt_euro_r_3m",
        "nb_contacts_accueil_service",
    ]
    expected_categorical_cols = [
        "dummy:iac_epa:03",
        "dummy:iac_epa:01",
        "dummy:iac_epa:02",
    ]
    expected_target = "souscription_produit_1m"

    print("\nüîç 5. COMPARAISON AVEC LE TEST PR√âC√âDENT...")

    # V√©rifier pr√©sence des colonnes
    missing_seq_cols = [col for col in expected_sequence_cols if col not in all_columns]
    missing_cat_cols = [
        col for col in expected_categorical_cols if col not in all_columns
    ]
    target_present = expected_target in all_columns

    # Chercher des colonnes similaires si manquantes
    similar_cols = {}
    for missing_col in missing_seq_cols + missing_cat_cols:
        # Recherche par substring
        similar = [
            col
            for col in all_columns
            if any(part in col.lower() for part in missing_col.lower().split("_")[:2])
        ]
        if similar:
            similar_cols[missing_col] = similar[:3]  # Top 3 similaires

    # === ANALYSE TYPES ET QUALIT√â ===
    print("\nüìä 6. ANALYSE TYPES ET QUALIT√â...")

    column_analysis = {}
    for col in all_columns[:50]:  # Analyser les 50 premi√®res colonnes
        try:
            col_info = {
                "dtype": str(df_structure_sample[col].dtype),
                "non_null_count": int(df_structure_sample[col].count()),
                "null_percentage": round(
                    (df_structure_sample[col].isnull().sum() / len(df_structure_sample))
                    * 100,
                    2,
                ),
                "unique_values": int(df_structure_sample[col].nunique())
                if df_structure_sample[col].nunique() < 1000
                else "1000+",
            }

            # Ajouter des stats selon le type
            if pd.api.types.is_numeric_dtype(df_structure_sample[col]):
                col_info["mean"] = (
                    round(float(df_structure_sample[col].mean()), 2)
                    if df_structure_sample[col].mean()
                    == df_structure_sample[col].mean()
                    else "NaN"
                )
                col_info["std"] = (
                    round(float(df_structure_sample[col].std()), 2)
                    if df_structure_sample[col].std() == df_structure_sample[col].std()
                    else "NaN"
                )

            column_analysis[col] = col_info

        except Exception as e:
            column_analysis[col] = {"error": str(e)}

    # === ESTIMATION VOLUM√âTRIE 2024 ===
    print("\nüìà 7. ESTIMATION VOLUM√âTRIE 2024...")

    estimated_total_rows_2024 = total_estimated_size
    estimated_total_cols = len(all_columns)
    estimated_memory_gb = (estimated_total_rows_2024 * estimated_total_cols * 8) / (
        1024**3
    )  # 8 bytes par valeur approximatif

    volumetry_estimation = {
        "estimated_total_rows_2024": estimated_total_rows_2024,
        "estimated_total_columns": estimated_total_cols,
        "estimated_memory_gb": round(estimated_memory_gb, 2),
        "estimated_processing_time_hours": round(
            estimated_total_rows_2024 / 50000, 1
        ),  # Approximation bas√©e sur votre test
        "recommended_chunk_size": min(10000, estimated_total_rows_2024 // 10),
    }

else:
    print("‚ùå Aucun √©chantillon disponible pour analyse")
    column_analysis = {}
    volumetry_estimation = {}

# === CONSOLIDATION R√âSULTATS ===
print("\nüìã 8. CONSOLIDATION DES R√âSULTATS...")

final_analysis = {
    "analysis_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "partitions_analysis": analysis_results,
    "columns_comparison": {
        "total_columns_found": len(all_columns)
        if df_structure_sample is not None
        else 0,
        "expected_sequence_cols": expected_sequence_cols,
        "expected_categorical_cols": expected_categorical_cols,
        "expected_target": expected_target,
        "missing_sequence_cols": missing_seq_cols
        if df_structure_sample is not None
        else [],
        "missing_categorical_cols": missing_cat_cols
        if df_structure_sample is not None
        else [],
        "target_present": target_present if df_structure_sample is not None else False,
        "similar_columns_found": similar_cols
        if df_structure_sample is not None
        else {},
    },
    "data_quality": column_analysis,
    "volumetry_2024": volumetry_estimation,
    "recommendations": [],
}

# === G√âN√âRATION RECOMMANDATIONS ===
print("\nüí° 9. G√âN√âRATION RECOMMANDATIONS...")

recommendations = []

# Recommandations bas√©es sur l'analyse
if df_structure_sample is not None:
    if missing_seq_cols:
        recommendations.append(
            f"‚ö†Ô∏è Colonnes s√©quentielles manquantes: {missing_seq_cols}. V√©rifier noms ou utiliser alternatives."
        )

    if missing_cat_cols:
        recommendations.append(
            f"‚ö†Ô∏è Colonnes cat√©gorielles manquantes: {missing_cat_cols}. Adapter le pipeline."
        )

    if not target_present:
        recommendations.append(
            f"‚ùå Target '{expected_target}' non trouv√©e. Pipeline non applicable sans adaptation."
        )

    if estimated_memory_gb > 16:
        recommendations.append(
            f"üíæ Volume important ({estimated_memory_gb:.1f}GB). Recommand√©: processing par chunks."
        )

    if estimated_total_rows_2024 > 100000:
        recommendations.append(
            f"‚è±Ô∏è Volume important ({estimated_total_rows_2024:,} lignes). Temps d'entra√Ænement estim√©: {volumetry_estimation.get('estimated_processing_time_hours', '?')}h."
        )

    if len(all_columns) > 500:
        recommendations.append(
            f"üìä Beaucoup de colonnes ({len(all_columns)}). Recommand√©: s√©lection features pertinentes."
        )

final_analysis["recommendations"] = recommendations

# === CR√âATION DATASET DE SORTIE ===
print("\nüíæ 10. SAUVEGARDE R√âSULTATS...")

# Convertir en DataFrame pour Dataiku
results_rows = []

# R√©sum√© global
results_rows.append(
    {
        "analysis_type": "SUMMARY",
        "partition": "ALL_2024",
        "metric": "estimated_total_rows",
        "value": estimated_total_rows_2024,
        "details": json.dumps(volumetry_estimation),
    }
)

# D√©tails par partition
for partition, details in analysis_results.items():
    results_rows.append(
        {
            "analysis_type": "PARTITION",
            "partition": partition,
            "metric": "rows_analyzed",
            "value": details.get("rows_sampled", 0),
            "details": json.dumps(details),
        }
    )

# Colonnes manquantes
for col in missing_seq_cols + missing_cat_cols:
    results_rows.append(
        {
            "analysis_type": "MISSING_COLUMN",
            "partition": "ALL",
            "metric": col,
            "value": 0,
            "details": json.dumps(similar_cols.get(col, [])),
        }
    )

# Recommandations
for i, rec in enumerate(recommendations):
    results_rows.append(
        {
            "analysis_type": "RECOMMENDATION",
            "partition": "ALL",
            "metric": f"recommendation_{i + 1}",
            "value": 1,
            "details": rec,
        }
    )

# DataFrame final
df_results = pd.DataFrame(results_rows)

# Ajouter m√©tadonn√©es
df_results["analysis_date"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
df_results["compatible_with_test"] = target_present and len(missing_seq_cols) == 0

print(f"‚úÖ R√©sultats pr√©par√©s: {len(df_results)} lignes d'analyse")

# === √âCRITURE OUTPUT DATAIKU ===
try:
    output_dataset.write_with_schema(df_results)
    print("‚úÖ R√©sultats sauvegard√©s dans T4REC_ANALYSIS_PHASE1")
except Exception as e:
    print(f"‚ö†Ô∏è Erreur sauvegarde: {e}")
    print("üìä R√©sultats disponibles en m√©moire:")
    print(df_results.head())

# === R√âSUM√â FINAL ===
print("\n" + "=" * 70)
print("üéØ R√âSUM√â PHASE 1 - EXPLORATION TERMIN√âE")
print("=" * 70)

if df_structure_sample is not None:
    print(
        f"üìä Dataset: {len(all_columns)} colonnes, ~{estimated_total_rows_2024:,} lignes 2024"
    )
    print(
        f"üéØ Compatibilit√© test: {'‚úÖ OUI' if target_present and len(missing_seq_cols) == 0 else '‚ùå NON'}"
    )
    print(f"üíæ M√©moire estim√©e: {estimated_memory_gb:.1f}GB")
    print(
        f"‚è±Ô∏è Temps estim√©: {volumetry_estimation.get('estimated_processing_time_hours', '?')}h"
    )

    if recommendations:
        print(f"\n‚ö†Ô∏è {len(recommendations)} recommandations importantes:")
        for rec in recommendations[:3]:
            print(f"   ‚Ä¢ {rec}")

    print(
        f"\nüìã Prochaine √©tape: {'Phase 2 - Adaptation pipeline' if target_present else 'Corriger colonnes manquantes'}"
    )

else:
    print("‚ùå √âchec analyse - V√©rifier connexion dataset")

print("=" * 70)
