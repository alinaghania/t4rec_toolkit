# === ANALYSE RAPIDE Ã‰CHANTILLON - BASE_SCORE_COMPLETE_prepared ===
# Version optimisÃ©e pour analyser rapidement la structure sans surcharger

import dataiku
import pandas as pd
import numpy as np
from datetime import datetime
import json
import warnings

warnings.filterwarnings("ignore")

print("ðŸ” ANALYSE RAPIDE Ã‰CHANTILLON - BASE_SCORE_COMPLETE_prepared")
print("=" * 65)

# === CONFIGURATION Ã‰CHANTILLONNAGE ===
SAMPLE_SIZE = 1000  # Petit Ã©chantillon pour rapiditÃ©
MAX_PARTITIONS_TO_CHECK = 2  # Maximum 2 partitions pour aller vite

# === CONNEXION DATASETS DATAIKU ===
print("\nðŸ“Š 1. CONNEXION DATASET...")

# Input : Table partitionnÃ©e source
input_dataset = dataiku.Dataset("BASE_SCORE_COMPLETE_prepared")

# Output : RÃ©sultats d'analyse rapide
output_dataset = dataiku.Dataset("T4REC_SAMPLE_ANALYSIS")

# === RÃ‰CUPÃ‰RATION Ã‰CHANTILLON RAPIDE ===
print(f"\nðŸ”¬ 2. Ã‰CHANTILLONNAGE RAPIDE ({SAMPLE_SIZE} lignes)...")


def get_quick_sample():
    """RÃ©cupÃ¨re rapidement un Ã©chantillon reprÃ©sentatif"""
    try:
        # MÃ©thode 1 : Essayer de prendre directement un Ã©chantillon
        print("   ðŸ“Š Tentative Ã©chantillon direct...")
        df_sample = input_dataset.get_dataframe(limit=SAMPLE_SIZE)
        print(f"   âœ… Ã‰chantillon obtenu: {df_sample.shape}")
        return df_sample, "direct"

    except Exception as e:
        print(f"   âš ï¸ Ã‰chantillon direct Ã©chouÃ©: {e}")

        try:
            # MÃ©thode 2 : Essayer via une partition rÃ©cente
            print("   ðŸ“Š Tentative via partitions...")
            partitions = input_dataset.list_partitions()

            # Prendre les partitions les plus rÃ©centes (probablement 2024)
            recent_partitions = sorted(partitions)[-MAX_PARTITIONS_TO_CHECK:]
            print(f"   ðŸ“… Partitions testÃ©es: {recent_partitions}")

            for partition in recent_partitions:
                try:
                    df_sample = input_dataset.get_dataframe(
                        partition=partition, limit=SAMPLE_SIZE
                    )
                    if len(df_sample) > 0:
                        print(
                            f"   âœ… Ã‰chantillon obtenu via {partition}: {df_sample.shape}"
                        )
                        return df_sample, f"partition_{partition}"
                except Exception as pe:
                    print(f"   âš ï¸ Partition {partition} Ã©chouÃ©e: {pe}")
                    continue

            # Si aucune partition ne marche
            print("   âŒ Aucune partition accessible")
            return None, "failed"

        except Exception as e2:
            print(f"   âŒ MÃ©thode partitions Ã©chouÃ©e: {e2}")
            return None, "failed"


# RÃ©cupÃ©rer l'Ã©chantillon
df_sample, method = get_quick_sample()

if df_sample is None:
    print("âŒ IMPOSSIBLE D'OBTENIR UN Ã‰CHANTILLON")
    print("ðŸ” VÃ©rifiez les permissions d'accÃ¨s au dataset")
    exit()

print(f"âœ… Ã‰chantillon rÃ©cupÃ©rÃ© via: {method}")

# === ANALYSE RAPIDE STRUCTURE ===
print(f"\nðŸ—ï¸ 3. ANALYSE STRUCTURE ({df_sample.shape})...")

# Infos de base
total_columns = len(df_sample.columns)
total_rows = len(df_sample)
memory_mb = df_sample.memory_usage(deep=True).sum() / 1024 / 1024

print(f"ðŸ“Š Dimensions: {total_rows:,} lignes Ã— {total_columns} colonnes")
print(f"ðŸ’¾ MÃ©moire Ã©chantillon: {memory_mb:.1f} MB")

# === COMPARAISON AVEC TEST PRÃ‰CÃ‰DENT ===
print("\nðŸ” 4. COMPATIBILITÃ‰ AVEC VOTRE TEST...")

# Colonnes attendues du test pipeline_t4rec_training.py
expected_columns = {
    "sequences": [
        "nbchqemigliss_m12",
        "nb_automobile_12dm",
        "mntecscrdimm",
        "mnt_euro_r_3m",
        "nb_contacts_accueil_service",
    ],
    "categoriques": ["dummy:iac_epa:03", "dummy:iac_epa:01", "dummy:iac_epa:02"],
    "target": "souscription_produit_1m",
}

all_columns = list(df_sample.columns)
compatibility_report = {}

# VÃ©rifier chaque type de colonne
for col_type, col_list in expected_columns.items():
    if col_type == "target":
        # Target est une seule colonne
        found = col_list in all_columns
        compatibility_report[col_type] = {
            "expected": col_list,
            "found": found,
            "missing": [] if found else [col_list],
        }
        print(f"ðŸŽ¯ Target '{col_list}': {'âœ… TROUVÃ‰E' if found else 'âŒ MANQUANTE'}")
    else:
        # Listes de colonnes
        missing = [col for col in col_list if col not in all_columns]
        found_count = len(col_list) - len(missing)
        compatibility_report[col_type] = {
            "expected": col_list,
            "found_count": found_count,
            "missing": missing,
        }
        print(f"ðŸ“‹ {col_type.capitalize()}: {found_count}/{len(col_list)} trouvÃ©es")
        if missing:
            print(f"   âŒ Manquantes: {missing}")

# === RECHERCHE COLONNES SIMILAIRES ===
print("\nðŸ” 5. RECHERCHE COLONNES SIMILAIRES...")


def find_similar_columns(missing_col, all_cols, max_results=3):
    """Trouve des colonnes similaires par nom"""
    similar = []
    missing_lower = missing_col.lower()

    # Recherche par mots-clÃ©s
    keywords = missing_lower.replace(":", "_").split("_")

    for col in all_cols:
        col_lower = col.lower()
        # Si au moins 2 mots-clÃ©s correspondent
        matches = sum(1 for kw in keywords if kw in col_lower and len(kw) > 2)
        if matches >= 2:
            similar.append(col)

    return similar[:max_results]


similar_suggestions = {}
all_missing = []

for col_type, report in compatibility_report.items():
    if "missing" in report and report["missing"]:
        all_missing.extend(report["missing"])

for missing_col in all_missing:
    similar = find_similar_columns(missing_col, all_columns)
    if similar:
        similar_suggestions[missing_col] = similar
        print(f"ðŸ’¡ '{missing_col}' â†’ Similaires: {similar}")

# === ANALYSE QUALITÃ‰ RAPIDE ===
print("\nðŸ“Š 6. QUALITÃ‰ DES DONNÃ‰ES (TOP 20 colonnes)...")

quality_summary = {}
sample_columns = all_columns[:20]  # Analyser seulement les 20 premiÃ¨res pour rapiditÃ©

for col in sample_columns:
    try:
        null_pct = round((df_sample[col].isnull().sum() / len(df_sample)) * 100, 1)
        unique_count = df_sample[col].nunique()
        dtype = str(df_sample[col].dtype)

        quality_summary[col] = {
            "null_percentage": null_pct,
            "unique_values": unique_count,
            "dtype": dtype,
        }

        # Affichage condensÃ©
        status = "ðŸŸ¢" if null_pct < 10 else "ðŸŸ¡" if null_pct < 50 else "ðŸ”´"
        print(
            f"   {status} {col[:30]:<30} | {null_pct:>5.1f}% null | {unique_count:>4} uniques | {dtype}"
        )

    except Exception as e:
        quality_summary[col] = {"error": str(e)}

# === ESTIMATION VOLUMÃ‰TRIE RAPIDE ===
print("\nðŸ“ˆ 7. ESTIMATION VOLUMÃ‰TRIE 2024...")

# Estimation trÃ¨s approximative basÃ©e sur l'Ã©chantillon
if method.startswith("partition"):
    # Si on a une partition, estimer pour 12 mois
    estimated_rows_2024 = total_rows * 12
elif method == "direct":
    # Si Ã©chantillon direct, assumer que c'est reprÃ©sentatif
    full_dataset_estimate = total_rows * 100  # Facteur approximatif
    estimated_rows_2024 = full_dataset_estimate

estimated_memory_gb = (estimated_rows_2024 * total_columns * 8) / (1024**3)
estimated_processing_hours = estimated_rows_2024 / 10000  # TrÃ¨s approximatif

print(f"ðŸ“Š Estimation 2024:")
print(f"   ðŸ“ˆ Lignes estimÃ©es: ~{estimated_rows_2024:,}")
print(f"   ðŸ’¾ MÃ©moire estimÃ©e: ~{estimated_memory_gb:.1f} GB")
print(f"   â±ï¸ Temps traitement estimÃ©: ~{estimated_processing_hours:.1f}h")

# === RECOMMANDATIONS RAPIDES ===
print("\nðŸ’¡ 8. RECOMMANDATIONS...")

recommendations = []

# CompatibilitÃ©
target_ok = compatibility_report.get("target", {}).get("found", False)
seq_missing = len(compatibility_report.get("sequences", {}).get("missing", []))
cat_missing = len(compatibility_report.get("categoriques", {}).get("missing", []))

if not target_ok:
    recommendations.append(
        "âŒ CRITIQUE: Target manquante - Pipeline non utilisable tel quel"
    )
if seq_missing > 2:
    recommendations.append(
        f"âš ï¸ {seq_missing} colonnes sÃ©quentielles manquantes - Adapter le pipeline"
    )
if cat_missing > 1:
    recommendations.append(
        f"âš ï¸ {cat_missing} colonnes catÃ©gorielles manquantes - VÃ©rifier encodage"
    )

# Volume
if estimated_memory_gb > 32:
    recommendations.append(
        f"ðŸ’¾ Volume important ({estimated_memory_gb:.1f}GB) - Processing par chunks nÃ©cessaire"
    )
if estimated_processing_hours > 4:
    recommendations.append(
        f"â±ï¸ Traitement long ({estimated_processing_hours:.1f}h) - ConsidÃ©rer sous-Ã©chantillonnage"
    )

# QualitÃ©
high_null_cols = [
    col
    for col, info in quality_summary.items()
    if isinstance(info, dict) and info.get("null_percentage", 0) > 50
]
if high_null_cols:
    recommendations.append(
        f"ðŸ”´ {len(high_null_cols)} colonnes >50% nulls - VÃ©rifier qualitÃ©"
    )

# Afficher recommandations
for i, rec in enumerate(recommendations, 1):
    print(f"   {i}. {rec}")

# === VERDICT FINAL ===
print("\n" + "=" * 65)
print("ðŸŽ¯ VERDICT ANALYSE RAPIDE")
print("=" * 65)

# Score de compatibilitÃ©
compatibility_score = 0
if target_ok:
    compatibility_score += 40
compatibility_score += max(0, (5 - seq_missing) * 8)  # 8 points par colonne seq trouvÃ©e
compatibility_score += max(0, (3 - cat_missing) * 4)  # 4 points par colonne cat trouvÃ©e

if compatibility_score >= 80:
    verdict = "ðŸŸ¢ EXCELLENT - Pipeline directement applicable"
elif compatibility_score >= 60:
    verdict = "ðŸŸ¡ BON - Adaptations mineures nÃ©cessaires"
elif compatibility_score >= 40:
    verdict = "ðŸŸ  MOYEN - Adaptations importantes nÃ©cessaires"
else:
    verdict = "ðŸ”´ PROBLÃ‰MATIQUE - Refonte majeure nÃ©cessaire"

print(f"ðŸ“Š Score compatibilitÃ©: {compatibility_score}/100")
print(f"ðŸŽ¯ Verdict: {verdict}")

# === SAUVEGARDE RÃ‰SULTATS RAPIDES ===
print(f"\nðŸ’¾ 9. SAUVEGARDE RÃ‰SULTATS...")

# CrÃ©er un DataFrame de rÃ©sultats condensÃ©
results_data = []

# RÃ©sumÃ© global
results_data.append(
    {
        "metric_type": "SUMMARY",
        "metric_name": "sample_size",
        "value": total_rows,
        "details": f"Columns: {total_columns}, Memory: {memory_mb:.1f}MB, Method: {method}",
    }
)

results_data.append(
    {
        "metric_type": "COMPATIBILITY",
        "metric_name": "compatibility_score",
        "value": compatibility_score,
        "details": verdict,
    }
)

# Colonnes manquantes
for missing_col in all_missing:
    results_data.append(
        {
            "metric_type": "MISSING_COLUMN",
            "metric_name": missing_col,
            "value": 0,
            "details": json.dumps(similar_suggestions.get(missing_col, [])),
        }
    )

# Recommandations
for i, rec in enumerate(recommendations):
    results_data.append(
        {
            "metric_type": "RECOMMENDATION",
            "metric_name": f"recommendation_{i + 1}",
            "value": 1,
            "details": rec,
        }
    )

# Estimations
results_data.append(
    {
        "metric_type": "ESTIMATION",
        "metric_name": "estimated_rows_2024",
        "value": estimated_rows_2024,
        "details": f"Memory: {estimated_memory_gb:.1f}GB, Time: {estimated_processing_hours:.1f}h",
    }
)

# CrÃ©er DataFrame
df_results = pd.DataFrame(results_data)
df_results["analysis_date"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
df_results["ready_for_pipeline"] = compatibility_score >= 60

print(f"âœ… {len(df_results)} rÃ©sultats prÃ©parÃ©s")

# Sauvegarder
try:
    output_dataset.write_with_schema(df_results)
    print("âœ… RÃ©sultats sauvegardÃ©s dans T4REC_SAMPLE_ANALYSIS")
except Exception as e:
    print(f"âš ï¸ Erreur sauvegarde: {e}")
    print("ðŸ“Š AperÃ§u rÃ©sultats:")
    print(df_results[["metric_type", "metric_name", "value"]].head(10))

print("\n" + "=" * 65)
if compatibility_score >= 60:
    print("ðŸš€ PRÃŠT POUR PHASE 2 - Adaptation pipeline T4Rec")
else:
    print("ðŸ”§ CORRECTIONS NÃ‰CESSAIRES avant Phase 2")
print("=" * 65)
