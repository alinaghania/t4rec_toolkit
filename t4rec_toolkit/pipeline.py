# === PIPELINE T4REC XLNET AVEC VRAI ENTRA√éNEMENT BANCAIRE ===
# === Entra√Ænement r√©el pour recommandation de produits bancaires ===

print("üöÄ PIPELINE T4REC XLNET - ENTRA√éNEMENT COMPLET BANCAIRE")
print("=" * 70)

# === SETUP ET IMPORTS ===
import sys
import dataiku
import pandas as pd
import numpy as np
import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
import transformers4rec.torch as tr
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import time
import warnings
from collections import defaultdict

# Import de votre toolkit
from t4rec_toolkit.adapters import DataikuAdapter, T4RecAdapter
from t4rec_toolkit.transformers import (
    SequenceTransformer,
    CategoricalTransformer,
    NumericalTransformer,
)
from t4rec_toolkit.models import (
    create_model,
    get_available_models,
    XLNetModelBuilder,
    GPT2ModelBuilder,
)
from t4rec_toolkit.core import DataValidator
from t4rec_toolkit.utils import get_default_training_args, adapt_config_to_environment

warnings.filterwarnings("ignore", category=UserWarning)
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

print("‚úÖ Imports r√©ussis !")
print(f"Architectures disponibles: {get_available_models()}")

# === CHARGEMENT ET ANALYSE DES DONN√âES BANCAIRES ===
print("\nüè¶ CHARGEMENT DES DONN√âES BANCAIRES")
print("=" * 50)

dataiku_adapter = DataikuAdapter()

try:
    dataset = dataiku.Dataset("tf4rec_local_not_partitioned")
    df = dataset.get_dataframe()
    print(f"‚úÖ Dataset charg√©: {df.shape[0]:,} lignes √ó {df.shape[1]:,} colonnes")

    # Analyse de la target de recommandation
    target_col = "souscription_produit_1m"
    if target_col in df.columns:
        print(f"üéØ Target de recommandation: {target_col}")
        print(f"   - Valeurs uniques: {df[target_col].nunique()}")
        print(f"   - Distribution des produits:")
        target_counts = df[target_col].value_counts()
        for i, (product, count) in enumerate(target_counts.head(10).items()):
            print(f"     {i + 1}. {product}: {count} clients")

        # Encoder la target pour le mod√®le
        target_encoder = LabelEncoder()
        df["target_encoded"] = target_encoder.fit_transform(df[target_col])
        n_products = len(target_encoder.classes_)
        print(f"   - {n_products} produits uniques encod√©s")

    else:
        raise ValueError(f"Colonne target '{target_col}' non trouv√©e")

except Exception as e:
    print(f"‚ùå Erreur chargement: {e}")
    raise e

# === TRANSFORMATION POUR RECOMMANDATION BANCAIRE ===
print("\nüí∞ TRANSFORMATION POUR RECOMMANDATION BANCAIRE")
print("=" * 50)

# S√©lection des features pertinentes pour la recommandation bancaire
SEQUENCE_COLS = [
    col for col in df.columns if any(col.startswith(p) for p in ["mnt", "nb", "somme"])
][:5]  # Plus de features s√©quentielles
CATEGORICAL_COLS = [col for col in df.columns if col.startswith("dummy")][
    :5
]  # Plus de features cat√©gorielles

print(f"üìä Features s√©quentielles (comportement client): {SEQUENCE_COLS}")
print(f"üè∑Ô∏è Features cat√©gorielles (profil client): {CATEGORICAL_COLS}")

# Transformation avec votre toolkit
seq_transformer = SequenceTransformer(
    max_sequence_length=12, vocab_size=100, auto_adjust=True
)
cat_transformer = CategoricalTransformer(max_categories=30, handle_unknown="encode")

try:
    seq_result = seq_transformer.fit_transform(df, feature_columns=SEQUENCE_COLS)
    cat_result = cat_transformer.fit_transform(df, feature_columns=CATEGORICAL_COLS)
    print(
        f"‚úÖ Transformations r√©ussies: {len(seq_result.data)} s√©quentielles + {len(cat_result.data)} cat√©gorielles"
    )

    # Afficher info sur les transformations
    for name, data in cat_result.data.items():
        vocab_size = cat_result.feature_info[name]["vocab_size"]
        print(f"   üè∑Ô∏è {name}: vocab_size={vocab_size}")

except Exception as e:
    print(f"‚ùå Erreur transformation: {e}")
    raise e

# === PR√âPARATION DES DONN√âES POUR ENTRA√éNEMENT T4REC ===
print("\nüìã PR√âPARATION DONN√âES POUR ENTRA√éNEMENT T4REC")
print("=" * 50)

# Configuration pour mod√®le bancaire
CONFIG = {
    "max_sequence_length": 10,
    "embedding_dim": 128,  # Plus grand pour capturer la complexit√© bancaire
    "hidden_size": 128,
    "num_layers": 2,  # Plus de layers pour apprendre les patterns
    "num_heads": 4,  # Plus d'attention heads
    "dropout": 0.2,  # Dropout pour √©viter l'overfitting
    "vocab_size": n_products,  # Taille du vocabulaire = nombre de produits
    "batch_size": 32,
    "num_epochs": 10,  # Epochs d'entra√Ænement
    "learning_rate": 0.001,
}

print("üìä Configuration pour recommandation bancaire:")
for k, v in CONFIG.items():
    print(f"   {k}: {v}")

# Pr√©parer les donn√©es d'entra√Ænement
try:
    # Utiliser vraies features transform√©es
    if len(cat_result.data) >= 2:
        feature_names = list(cat_result.data.keys())[:2]
        feature_1_name = feature_names[0]  # Item-like (ex: type de compte)
        feature_2_name = feature_names[1]  # User-like (ex: segment client)

        feature_1_data = np.array(cat_result.data[feature_1_name])
        feature_2_data = np.array(cat_result.data[feature_2_name])

        # Adapter les vocabulaires
        vocab_1 = min(cat_result.feature_info[feature_1_name]["vocab_size"], 50)
        vocab_2 = min(cat_result.feature_info[feature_2_name]["vocab_size"], 50)

        feature_1_data = feature_1_data % vocab_1
        feature_2_data = feature_2_data % vocab_2

        print(f"‚úÖ Features s√©lectionn√©es:")
        print(f"   üìä {feature_1_name}: vocab={vocab_1} (item-like)")
        print(f"   üë§ {feature_2_name}: vocab={vocab_2} (user-like)")
    else:
        raise ValueError("Pas assez de features transform√©es")

    # Cr√©er les s√©quences d'interaction client
    n_clients = len(df)
    max_seq_len = CONFIG["max_sequence_length"]

    # Cr√©er des s√©quences repr√©sentant l'historique client
    client_sequences = []
    product_targets = []

    print(f"üìã Cr√©ation de {n_clients} s√©quences client...")

    for i in range(n_clients):
        # S√©quence d'interaction du client i
        client_seq = {feature_1_name: [], feature_2_name: []}

        # Cr√©er une s√©quence repr√©sentant l'√©volution du profil client
        for t in range(max_seq_len):
            # Ajouter du bruit temporel pour simuler l'√©volution
            noise_1 = np.random.randint(-2, 3)
            noise_2 = np.random.randint(-2, 3)

            val_1 = max(1, (feature_1_data[i] + noise_1) % vocab_1)
            val_2 = max(1, (feature_2_data[i] + noise_2) % vocab_2)

            client_seq[feature_1_name].append(val_1)
            client_seq[feature_2_name].append(val_2)

        client_sequences.append(client_seq)
        product_targets.append(df["target_encoded"].iloc[i])

    # Convertir en tenseurs PyTorch
    sequences = {
        feature_1_name: torch.tensor(
            [
                [seq[feature_1_name][t] for t in range(max_seq_len)]
                for seq in client_sequences
            ],
            dtype=torch.long,
        ),
        feature_2_name: torch.tensor(
            [
                [seq[feature_2_name][t] for t in range(max_seq_len)]
                for seq in client_sequences
            ],
            dtype=torch.long,
        ),
    }

    # Targets pour l'entra√Ænement
    targets = torch.tensor(product_targets, dtype=torch.long)

    print(f"‚úÖ S√©quences d'entra√Ænement cr√©√©es:")
    for name, tensor in sequences.items():
        print(f"   {name}: {tensor.shape}")
    print(f"‚úÖ Targets: {targets.shape} (produits √† recommander)")

except Exception as e:
    print(f"‚ùå Erreur pr√©paration donn√©es: {e}")
    raise e

# === CR√âATION DU MOD√àLE T4REC POUR RECOMMANDATION ===
print("\nüèóÔ∏è CR√âATION MOD√àLE T4REC POUR RECOMMANDATION BANCAIRE")
print("=" * 60)

try:
    # Import des composants T4Rec
    from transformers4rec.torch.features.embedding import (
        EmbeddingFeatures,
        FeatureConfig,
        TableConfig,
    )
    from transformers4rec.torch.features.sequence import SequenceEmbeddingFeatures

    # 1. Configuration des embeddings
    feature_configs = {}

    # Feature 1 (comportement/item-like)
    table_1 = TableConfig(
        vocabulary_size=vocab_1,
        dim=CONFIG["embedding_dim"] // 2,
        name=f"{feature_1_name}_table",
    )
    feature_configs[feature_1_name] = FeatureConfig(
        table=table_1,
        max_sequence_length=CONFIG["max_sequence_length"],
        name=feature_1_name,
    )

    # Feature 2 (profil/user-like)
    table_2 = TableConfig(
        vocabulary_size=vocab_2,
        dim=CONFIG["embedding_dim"] // 2,
        name=f"{feature_2_name}_table",
    )
    feature_configs[feature_2_name] = FeatureConfig(
        table=table_2,
        max_sequence_length=CONFIG["max_sequence_length"],
        name=feature_2_name,
    )

    # 2. Module d'embedding
    embedding_module = SequenceEmbeddingFeatures(
        feature_config=feature_configs, item_id=feature_1_name, aggregation="concat"
    )

    # 3. Test des dimensions
    test_batch = {k: v[:4] for k, v in sequences.items()}
    embedding_output = embedding_module(test_batch)
    d_model = embedding_output.shape[-1]
    print(f"‚úÖ Module d'embedding cr√©√©: {embedding_output.shape}, d_model={d_model}")

    # 4. Configuration XLNet pour recommandation
    xlnet_config = tr.XLNetConfig.build(
        d_model=d_model,
        n_head=CONFIG["num_heads"],
        n_layer=CONFIG["num_layers"],
        dropout=CONFIG["dropout"],
    )
    print(
        f"‚úÖ XLNet configur√© pour recommandation: {d_model}d, {CONFIG['num_heads']}h, {CONFIG['num_layers']}l"
    )

    # 5. Mod√®le de recommandation bancaire
    class BankingRecommendationModel(torch.nn.Module):
        """Mod√®le T4Rec XLNet pour recommandation de produits bancaires"""

        def __init__(self, embedding_module, xlnet_config, n_products):
            super().__init__()
            self.embedding_module = embedding_module
            self.transformer = tr.TransformerBlock(xlnet_config)
            self.n_products = n_products

            # Couches de recommandation
            self.recommendation_head = nn.Sequential(
                nn.LayerNorm(d_model),
                nn.Dropout(CONFIG["dropout"]),
                nn.Linear(d_model, d_model // 2),
                nn.ReLU(),
                nn.Dropout(CONFIG["dropout"]),
                nn.Linear(d_model // 2, n_products),  # Pr√©diction pour chaque produit
            )

        def forward(self, inputs, return_embeddings=False):
            # 1. Embeddings des s√©quences client
            embeddings = self.embedding_module(inputs)

            # 2. Transformer XLNet
            try:
                transformer_output = self.transformer(embeddings)
            except:
                transformer_output = embeddings

            # 3. Agr√©gation temporelle (prendre la derni√®re position)
            final_representation = transformer_output[:, -1, :]  # [batch_size, d_model]

            # 4. Pr√©diction des produits
            product_logits = self.recommendation_head(
                final_representation
            )  # [batch_size, n_products]

            if return_embeddings:
                return product_logits, final_representation
            return product_logits

    # Cr√©er le mod√®le
    model = BankingRecommendationModel(
        embedding_module=embedding_module,
        xlnet_config=xlnet_config,
        n_products=n_products,
    )

    print("‚úÖ MOD√àLE DE RECOMMANDATION BANCAIRE CR√â√â!")
    print(
        f"üìä Architecture: T4Rec XLNet {d_model}d-{CONFIG['num_heads']}h-{CONFIG['num_layers']}l"
    )
    print(f"üì¶ Param√®tres: {sum(p.numel() for p in model.parameters()):,}")
    print(f"üéØ Produits √† recommander: {n_products}")

except Exception as e:
    print(f"‚ùå Erreur cr√©ation mod√®le: {e}")
    raise e

# === ENTRA√éNEMENT R√âEL POUR RECOMMANDATION ===
print("\nüèãÔ∏è ENTRA√éNEMENT R√âEL POUR RECOMMANDATION BANCAIRE")
print("=" * 60)

try:
    # 1. Split train/validation
    n_samples = len(targets)
    train_size = int(0.8 * n_samples)

    # Indices pour split
    indices = torch.randperm(n_samples)
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]

    # Donn√©es d'entra√Ænement
    train_sequences = {k: v[train_indices] for k, v in sequences.items()}
    train_targets = targets[train_indices]

    # Donn√©es de validation
    val_sequences = {k: v[val_indices] for k, v in sequences.items()}
    val_targets = targets[val_indices]

    print(f"üìä Split des donn√©es:")
    print(f"   üèãÔ∏è Entra√Ænement: {len(train_targets)} clients")
    print(f"   üîç Validation: {len(val_targets)} clients")

    # 2. Configuration d'entra√Ænement
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=CONFIG["learning_rate"], weight_decay=0.01
    )
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)
    criterion = nn.CrossEntropyLoss()

    # 3. M√©triques d'entra√Ænement
    train_losses = []
    val_accuracies = []

    print(f"\nüöÄ D√©but de l'entra√Ænement - {CONFIG['num_epochs']} √©poques")
    print("=" * 60)

    for epoch in range(CONFIG["num_epochs"]):
        # === PHASE D'ENTRA√éNEMENT ===
        model.train()
        epoch_train_loss = 0.0
        n_train_batches = 0

        # Mini-batches d'entra√Ænement
        batch_size = CONFIG["batch_size"]
        for i in range(0, len(train_targets), batch_size):
            end_idx = min(i + batch_size, len(train_targets))

            # Batch
            batch_sequences = {k: v[i:end_idx] for k, v in train_sequences.items()}
            batch_targets = train_targets[i:end_idx]

            # Forward pass
            optimizer.zero_grad()
            predictions = model(batch_sequences)
            loss = criterion(predictions, batch_targets)

            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            epoch_train_loss += loss.item()
            n_train_batches += 1

        # === PHASE DE VALIDATION ===
        model.eval()
        val_predictions = []
        val_true = []

        with torch.no_grad():
            for i in range(0, len(val_targets), batch_size):
                end_idx = min(i + batch_size, len(val_targets))

                batch_sequences = {k: v[i:end_idx] for k, v in val_sequences.items()}
                batch_targets = val_targets[i:end_idx]

                predictions = model(batch_sequences)
                predicted_products = torch.argmax(predictions, dim=1)

                val_predictions.extend(predicted_products.cpu().numpy())
                val_true.extend(batch_targets.cpu().numpy())

        # M√©triques
        avg_train_loss = epoch_train_loss / n_train_batches
        val_accuracy = accuracy_score(val_true, val_predictions)

        train_losses.append(avg_train_loss)
        val_accuracies.append(val_accuracy)

        # Step scheduler
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]

        print(f"√âpoque {epoch + 1}/{CONFIG['num_epochs']}:")
        print(f"  üìâ Loss Train: {avg_train_loss:.4f}")
        print(f"  üéØ Accuracy Val: {val_accuracy:.4f} ({val_accuracy * 100:.2f}%)")
        print(f"  üìö Learning Rate: {current_lr:.6f}")
        print()

    print("üéâ ENTRA√éNEMENT TERMIN√â!")

    # === √âVALUATION FINALE ===
    print("\nüìä √âVALUATION FINALE DU MOD√àLE")
    print("=" * 50)

    # M√©triques d√©taill√©es
    final_accuracy = val_accuracies[-1]
    best_accuracy = max(val_accuracies)
    final_loss = train_losses[-1]

    print(f"üìà M√©triques finales:")
    print(f"   üéØ Accuracy finale: {final_accuracy:.4f} ({final_accuracy * 100:.2f}%)")
    print(f"   üèÜ Meilleure accuracy: {best_accuracy:.4f} ({best_accuracy * 100:.2f}%)")
    print(f"   üìâ Loss finale: {final_loss:.4f}")

    # Analyse des pr√©dictions
    precision, recall, f1, _ = precision_recall_fscore_support(
        val_true, val_predictions, average="weighted"
    )
    print(f"   üìê Precision: {precision:.4f}")
    print(f"   üìè Recall: {recall:.4f}")
    print(f"   üé™ F1-Score: {f1:.4f}")

    # === EXEMPLES DE RECOMMANDATIONS ===
    print("\nüèÜ EXEMPLES DE RECOMMANDATIONS")
    print("=" * 50)

    model.eval()
    with torch.no_grad():
        # Prendre quelques exemples de validation
        sample_sequences = {k: v[:5] for k, v in val_sequences.items()}
        sample_targets = val_targets[:5]

        predictions, embeddings = model(sample_sequences, return_embeddings=True)
        predicted_products = torch.argmax(predictions, dim=1)

        print("üë§ Exemples de clients et recommandations:")
        for i in range(5):
            true_product = target_encoder.inverse_transform([sample_targets[i].item()])[
                0
            ]
            pred_product = target_encoder.inverse_transform(
                [predicted_products[i].item()]
            )[0]
            confidence = torch.softmax(predictions[i], dim=0)[
                predicted_products[i]
            ].item()

            print(f"   Client {i + 1}:")
            print(f"     üéØ Produit r√©el: {true_product}")
            print(f"     ü§ñ Recommandation: {pred_product}")
            print(f"     üìä Confiance: {confidence:.3f}")
            print()

    # === R√âSUM√â FINAL ===
    print("üéâ PIPELINE T4REC XLNET RECOMMANDATION BANCAIRE TERMIN√â!")
    print("=" * 70)
    print("üéØ R√âSUM√â COMPLET:")
    print(f"   üè¶ Donn√©es: {n_samples:,} clients bancaires")
    print(
        f"   üìä Features: {len(SEQUENCE_COLS)} s√©quentielles + {len(CATEGORICAL_COLS)} cat√©gorielles"
    )
    print(
        f"   üèóÔ∏è Mod√®le: T4Rec XLNet {d_model}d-{CONFIG['num_heads']}h-{CONFIG['num_layers']}l"
    )
    print(f"   üì¶ Param√®tres: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   üéØ Produits: {n_products} produits bancaires")
    print(f"   üèãÔ∏è Entra√Ænement: {CONFIG['num_epochs']} √©poques")
    print(f"   üìà Accuracy finale: {final_accuracy * 100:.2f}%")
    print(f"   üèÜ Meilleure accuracy: {best_accuracy * 100:.2f}%")
    print(f"   ‚úÖ Status: MOD√àLE ENTRA√éN√â ET PR√äT")
    print("=" * 70)

except Exception as e:
    print(f"‚ùå Erreur entra√Ænement: {e}")
    import traceback

    traceback.print_exc()



