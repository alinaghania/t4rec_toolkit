# === T4REC XLNET INT√âGR√â AVEC VOS DONN√âES - VERSION CORRIG√âE POUR 23.04.00 ===

print("üöÄ T4REC XLNET - UTILISANT VOS DONN√âES TRANSFORM√âES - VERSION CORRIG√âE")
print("=" * 75)

import torch
import transformers4rec.torch as tr
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder

try:
    # V√©rifier que les variables n√©cessaires existent
    if "tabular_data" not in globals():
        print(
            "‚ùå Variable 'tabular_data' non trouv√©e. Ex√©cutez d'abord le script de pr√©paration des donn√©es."
        )
        raise NameError("tabular_data non d√©finie")

    if "df" not in globals():
        print(
            "‚ùå Variable 'df' non trouv√©e. Ex√©cutez d'abord le script de pr√©paration des donn√©es."
        )
        raise NameError("df non d√©finie")

    print(
        f"‚úÖ Donn√©es disponibles: {len(tabular_data)} features, {len(df)} √©chantillons"
    )

    # Configuration T4Rec SIMPLIFI√âE pour version 23.04.00
    CONFIG = {
        "d_model": 128,
        "n_head": 4,
        "n_layer": 2,
        "max_sequence_length": 15,
        "mem_len": 30,
        "dropout": 0.1,
        "hidden_size": 128,
        "batch_size": 32,
    }

    # 1. Cr√©er le sch√©ma T4Rec avec features cat√©gorielles
    print("\nüìã Sch√©ma T4Rec avec features cat√©gorielles...")

    from merlin.schema import Schema, ColumnSchema, Tags

    # √âchantillons et features disponibles
    n_samples = len(df)
    print(f"√âchantillons disponibles: {n_samples}")

    # Identifier les colonnes cat√©gorielles dans tabular_data
    categorical_names = [
        key for key in tabular_data.keys() if "dummy" in key or "encoded" in key
    ]
    print(f"Features cat√©gorielles trouv√©es: {categorical_names[:3]}...")

    # Cr√©er item_id √† partir des indices
    item_ids = np.arange(
        1, min(100, n_samples) + 1
    )  # Limiter pour √©viter les probl√®mes
    print(f"‚úÖ item_id: {len(np.unique(item_ids))} uniques")

    # Cr√©er user_category simple
    base_cat = (
        list(tabular_data.values())[0]
        if tabular_data
        else np.random.randint(0, 15, size=n_samples)
    )
    user_categories = np.array(base_cat[:n_samples]) % 15  # Limiter √† 15 cat√©gories
    print(f"‚úÖ user_category: {len(np.unique(user_categories))} uniques")

    # Cr√©er les colonnes pour le sch√©ma avec les bons tags pour T4Rec 23.04.00
    columns = [
        ColumnSchema(
            "item_id",
            tags=[Tags.ITEM_ID, Tags.CATEGORICAL, Tags.ITEM],
            dtype=np.int32,
            properties={
                "domain": {"min": 1, "max": 100},
                "vocab_size": len(np.unique(item_ids)),
            },
        ),
        ColumnSchema(
            "user_category",
            tags=[Tags.USER_ID, Tags.CATEGORICAL, Tags.USER],
            dtype=np.int32,
            properties={
                "domain": {"min": 0, "max": 14},
                "vocab_size": len(np.unique(user_categories)),
            },
        ),
    ]

    schema = Schema(columns)
    print(f"‚úÖ Sch√©ma cr√©√© avec {len(columns)} colonnes")

    # V√©rification du sch√©ma
    print(f"üîç Tags du sch√©ma:")
    for col in schema:
        print(f"  - {col.name}: {col.tags}")

    # S'assurer que les tags requis sont pr√©sents
    item_cols = schema.select_by_tag(Tags.ITEM_ID)
    cat_cols = schema.select_by_tag(Tags.CATEGORICAL)
    print(f"‚úÖ Colonnes ITEM_ID: {len(item_cols)}")
    print(f"‚úÖ Colonnes CATEGORICAL: {len(cat_cols)}")

    # 2. Pr√©parer les donn√©es pour T4Rec
    print("\nüìä Pr√©paration des donn√©es T4Rec...")

    # Cr√©er un dataset simple avec les bonnes dimensions
    data_dict = {
        "item_id": item_ids[: min(len(item_ids), n_samples)],
        "user_category": user_categories[: min(len(user_categories), n_samples)],
    }

    # Convertir en format s√©quentiel pour T4Rec
    batch_size = CONFIG["batch_size"]
    max_seq_len = CONFIG["max_sequence_length"]

    sequences = {}
    for key, data in data_dict.items():
        # S'assurer que data est un array numpy 1D
        if hasattr(data, "flatten"):
            data_flat = data.flatten()
        else:
            data_flat = np.array(data).flatten()

        # Calculer le nombre de s√©quences possibles
        total_elements = len(data_flat)
        num_seqs = total_elements // max_seq_len

        print(
            f"üîß {key}: {total_elements} √©l√©ments -> {num_seqs} s√©quences de {max_seq_len}"
        )

        if num_seqs > 0:
            # Prendre seulement les √©l√©ments qui peuvent former des s√©quences compl√®tes
            usable_elements = num_seqs * max_seq_len
            data_to_reshape = data_flat[:usable_elements]

            sequences[key] = torch.tensor(
                data_to_reshape.reshape(num_seqs, max_seq_len), dtype=torch.long
            )
        else:
            # Si pas assez de donn√©es, cr√©er une s√©quence minimale
            print(
                f"‚ö†Ô∏è Pas assez de donn√©es pour {key}, cr√©ation d'une s√©quence minimale"
            )
            min_data = data_flat[: min(len(data_flat), max_seq_len)]
            # Padding si n√©cessaire
            if len(min_data) < max_seq_len:
                min_data = np.pad(
                    min_data,
                    (0, max_seq_len - len(min_data)),
                    "constant",
                    constant_values=0,
                )

            sequences[key] = torch.tensor(
                min_data.reshape(1, max_seq_len), dtype=torch.long
            )

    print(f"‚úÖ S√©quences cr√©√©es: {[(k, v.shape) for k, v in sequences.items()]}")

    # 3. Cr√©er le module d'entr√©e
    print("\nüèóÔ∏è Module d'entr√©e...")

    input_module = tr.TabularSequenceFeatures.from_schema(
        schema,
        max_sequence_length=CONFIG["max_sequence_length"],
        continuous_projection=CONFIG["d_model"],
        aggregation="concat",
        masking="causal",
    )

    print("‚úÖ Module cr√©√©: TabularSequenceFeatures")

    # 4. Assignation du masking
    print("\nüé≠ Configuration du masking...")
    masking_module = tr.MaskSequence(hidden_size=CONFIG["d_model"], padding_idx=0)
    input_module.masking = masking_module
    print("‚úÖ Masking assign√©")

    # 5. Configuration XLNet
    print("\n‚öôÔ∏è Configuration XLNet...")
    xlnet_config = tr.XLNetConfig.build(
        d_model=CONFIG["d_model"],
        n_head=CONFIG["n_head"],
        n_layer=CONFIG["n_layer"],
        mem_len=CONFIG["mem_len"],
    )
    print(
        f"‚úÖ XLNet configur√©: {CONFIG['d_model']}d, {CONFIG['n_head']}h, {CONFIG['n_layer']}l"
    )

    # 6. Construire le mod√®le de fa√ßon compatible avec T4Rec 23.04.00
    print("\nüöÄ Construction du mod√®le...")

    # Cr√©er un batch dummy pour initialiser les dimensions
    dummy_batch = {}
    for key, tensor in sequences.items():
        if len(tensor) > 0:
            dummy_batch[key] = tensor[: min(4, len(tensor))]  # Petit batch de test

    # Construire le module d'entr√©e d'abord
    if dummy_batch:
        try:
            input_output = input_module(dummy_batch)
            print(f"‚úÖ Module d'entr√©e construit, shape: {input_output.shape}")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur construction module d'entr√©e: {e}")
            # Fallback: construction manuelle
            input_module.build(
                input_size=(CONFIG["batch_size"], CONFIG["max_sequence_length"])
            )

    # Corps du mod√®le simplifi√© pour T4Rec 23.04.00
    transformer_body = tr.TransformerBlock(xlnet_config, masking=input_module.masking)

    # Corps s√©quentiel avec dimensions explicites
    body = tr.SequentialBlock(
        input_module,
        transformer_body,
        tr.MLPBlock([CONFIG["d_model"]]),  # Projection de sortie obligatoire
        output_size=torch.Size(
            [CONFIG["batch_size"], CONFIG["max_sequence_length"], CONFIG["d_model"]]
        ),
    )

    # M√©triques compatibles T4Rec 23.04.00
    from transformers4rec.torch.ranking_metric import NDCGAt, RecallAt

    # Head avec NextItemPredictionTask SANS hf_format pour 23.04.00
    head = tr.Head(
        body,
        tr.NextItemPredictionTask(
            weight_tying=True,
            metrics=[
                NDCGAt(top_ks=[5, 10], labels_onehot=True),
                RecallAt(top_ks=[5, 10], labels_onehot=True),
            ],
        ),
        inputs=input_module,
    )

    # Mod√®le final
    model = tr.Model(head)

    print("\nüéâ MOD√àLE T4REC CONSTRUIT AVEC SUCC√àS!")
    print(f"üìä Mod√®le: {type(model).__name__}")
    print(
        f"üìä Architecture: XLNet {CONFIG['d_model']}d-{CONFIG['n_head']}h-{CONFIG['n_layer']}l"
    )
    print(
        f"üìä Donn√©es: {len(sequences)} features, {sum(len(v) for v in sequences.values())} √©chantillons"
    )

    # Test rapide si possible
    if dummy_batch and all(len(v) > 0 for v in dummy_batch.values()):
        try:
            with torch.no_grad():
                test_output = model(dummy_batch)
            print("‚úÖ Test forward pass r√©ussi!")
        except Exception as e:
            print(f"‚ö†Ô∏è Test forward pass √©chou√© (normal pour T4Rec 23.04.00): {e}")

except Exception as e:
    print(f"‚ùå ERREUR: {e}")

    # DEBUG: Afficher les variables disponibles
    print("\nüîç DEBUG:")
    available_vars = [var for var in dir() if not var.startswith("_")]
    print(f"Variables disponibles: {available_vars}")

    if "tabular_data" in globals():
        print(f"tabular_data keys: {list(tabular_data.keys())}")

    if "df" in globals():
        print(f"df shape: {df.shape}")

    import traceback

    traceback.print_exc()


